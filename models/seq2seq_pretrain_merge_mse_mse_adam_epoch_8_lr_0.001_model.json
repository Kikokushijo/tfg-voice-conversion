{"class_name": "Model", "keras_version": "1.2.2", "config": {"layers": [{"class_name": "InputLayer", "config": {"batch_input_shape": [null, 19962], "input_dtype": "int32", "sparse": false, "name": "src_spk_in"}, "inbound_nodes": [], "name": "src_spk_in"}, {"class_name": "InputLayer", "config": {"batch_input_shape": [null, 19962], "input_dtype": "int32", "sparse": false, "name": "trg_spk_in"}, "inbound_nodes": [], "name": "trg_spk_in"}, {"class_name": "InputLayer", "config": {"batch_input_shape": [null, 19962, 44], "input_dtype": "float32", "sparse": false, "name": "main_input"}, "inbound_nodes": [], "name": "main_input"}, {"class_name": "Embedding", "config": {"trainable": true, "name": "spk_index_embedding", "activity_regularizer": null, "W_constraint": null, "init": "uniform", "input_dtype": "int32", "mask_zero": false, "input_dim": 2, "batch_input_shape": [null, 19962], "W_regularizer": null, "dropout": 0.0, "output_dim": 5, "input_length": 19962}, "inbound_nodes": [[["src_spk_in", 0, 0]], [["trg_spk_in", 0, 0]], [["src_spk_in", 0, 0]], [["trg_spk_in", 0, 0]]], "name": "spk_index_embedding"}, {"class_name": "Merge", "config": {"name": "inputs_merge", "concat_axis": -1, "mode_type": "raw", "dot_axes": -1, "output_mask_type": "raw", "arguments": {}, "output_mask": null, "mode": "concat", "output_shape": null, "output_shape_type": "raw"}, "inbound_nodes": [[["main_input", 0, 0], ["spk_index_embedding", 0, 0], ["spk_index_embedding", 1, 0]]], "name": "inputs_merge"}, {"class_name": "PhasedLSTM", "config": {"U_regularizer": null, "name": "encoder_PLSTM", "inner_activation": "hard_sigmoid", "go_backwards": false, "activation": "tanh", "trainable": true, "input_dim": 54, "unroll": false, "consume_less": "gpu", "stateful": false, "init": "glorot_uniform", "inner_init": "orthogonal", "dropout_U": 0.0, "dropout_W": 0.0, "return_sequences": true, "alpha": 0.001, "b_regularizer": null, "W_regularizer": null, "output_dim": 256, "forget_bias_init": "one", "input_length": null}, "inbound_nodes": [[["inputs_merge", 0, 0]]], "name": "encoder_PLSTM"}, {"class_name": "Lambda", "config": {"function": ["c\u0001\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0004\u0000\u0000\u0000C \u0000\u0000s\u0010\u0000\u0000\u0000t\u0000\u0000|\u0000\u0000d\u0001\u0000d\u0002\u0000\u0083\u0001\u0001S(\u0003\u0000\u0000\u0000Nt\u0004\u0000\u0000\u0000axesi\u0001\u0000\u0000\u0000(\u0001\u0000\u0000\u0000t\u0007\u0000\u0000\u0000reverse(\u0001\u0000\u0000\u0000t\f\u0000\u0000\u0000input_tensor(\u0000\u0000\u0000\u0000(\u0000\u0000\u0000\u0000s#\u0000\u0000\u0000/veu/tfgtts1/tfglib/tfglib/utils.pyt\u0016\u0000\u0000\u0000reverse_encoder_output\u00e1\u0000\u0000\u0000s\u0002\u0000\u0000\u0000\u0000\u0001", null, null], "name": "reversed_encoder", "trainable": true, "function_type": "lambda", "arguments": {}, "output_shape": ["c\u0001\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0001\u0000\u0000\u0000C \u0000\u0000s\u0004\u0000\u0000\u0000|\u0000\u0000S(\u0001\u0000\u0000\u0000N(\u0000\u0000\u0000\u0000(\u0001\u0000\u0000\u0000t\u000b\u0000\u0000\u0000input_shape(\u0000\u0000\u0000\u0000(\u0000\u0000\u0000\u0000s#\u0000\u0000\u0000/veu/tfgtts1/tfglib/tfglib/utils.pyt\u0015\u0000\u0000\u0000reversed_output_shape\u00e5\u0000\u0000\u0000s\u0002\u0000\u0000\u0000\u0000\u0001", null, null], "output_shape_type": "lambda"}, "inbound_nodes": [[["encoder_PLSTM", 0, 0]]], "name": "reversed_encoder"}, {"class_name": "InputLayer", "config": {"batch_input_shape": [null, 19962, 44], "input_dtype": "float32", "sparse": false, "name": "feedback_in"}, "inbound_nodes": [], "name": "feedback_in"}, {"class_name": "Merge", "config": {"name": "merge_1", "concat_axis": -1, "mode_type": "raw", "dot_axes": -1, "output_mask_type": "raw", "arguments": {}, "output_mask": null, "mode": "concat", "output_shape": null, "output_shape_type": "raw"}, "inbound_nodes": [[["reversed_encoder", 0, 0], ["feedback_in", 0, 0], ["spk_index_embedding", 2, 0], ["spk_index_embedding", 3, 0]]], "name": "merge_1"}, {"class_name": "PhasedLSTM", "config": {"U_regularizer": null, "name": "decoder_PLSTM", "inner_activation": "hard_sigmoid", "go_backwards": false, "activation": "tanh", "trainable": true, "input_dim": 310, "unroll": false, "consume_less": "gpu", "stateful": false, "init": "glorot_uniform", "inner_init": "orthogonal", "dropout_U": 0.0, "dropout_W": 0.0, "return_sequences": true, "alpha": 0.001, "b_regularizer": null, "W_regularizer": null, "output_dim": 256, "forget_bias_init": "one", "input_length": null}, "inbound_nodes": [[["merge_1", 0, 0]]], "name": "decoder_PLSTM"}, {"class_name": "Dropout", "config": {"p": 0.5, "trainable": true, "name": "dropout_1"}, "inbound_nodes": [[["decoder_PLSTM", 0, 0]]], "name": "dropout_1"}, {"class_name": "PhasedLSTM", "config": {"U_regularizer": null, "name": "params_output", "inner_activation": "hard_sigmoid", "go_backwards": false, "activation": "linear", "trainable": true, "input_dim": 256, "unroll": false, "consume_less": "gpu", "stateful": false, "init": "glorot_uniform", "inner_init": "orthogonal", "dropout_U": 0.0, "dropout_W": 0.0, "return_sequences": true, "alpha": 0.001, "b_regularizer": null, "W_regularizer": null, "output_dim": 42, "forget_bias_init": "one", "input_length": null}, "inbound_nodes": [[["dropout_1", 0, 0]]], "name": "params_output"}, {"class_name": "TimeDistributed", "config": {"layer": {"class_name": "Dense", "config": {"W_constraint": null, "b_constraint": null, "name": "dense_1", "activity_regularizer": null, "trainable": true, "init": "glorot_uniform", "bias": true, "input_dim": 256, "b_regularizer": null, "W_regularizer": null, "activation": "sigmoid", "output_dim": 2}}, "trainable": true, "name": "flags_output"}, "inbound_nodes": [[["dropout_1", 0, 0]]], "name": "flags_output"}], "input_layers": [["main_input", 0, 0], ["src_spk_in", 0, 0], ["trg_spk_in", 0, 0], ["feedback_in", 0, 0]], "output_layers": [["params_output", 0, 0], ["flags_output", 0, 0]], "name": "model_1"}}